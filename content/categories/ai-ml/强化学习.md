---
title: "强化学习"
date: 2025-11-29T00:00:00+08:00
description: "强化学习 - 详细的技术文章和知识介绍"
draft: false
author: "AI知识库"
cover: "https://images.unsplash.com/photo-1516321318423-f06f85e504b3?w=800&h=400&fit=crop"
tags: ['人工智能', '机器学习', 'AI']
categories: ["ai-ml"]
theme: "light"
---

强化学习（英語：Reinforcement learning，簡稱RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。其关注点在于寻找（对未知领域的）探索和（对已有知识的）利用的平衡，强化学习中的“探索-利用”的交换，在多臂赌博机问题和有限MDP中研究得最多。
其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如博弈论、控制论、运筹学、信息论、仿真优化、多智能体系统、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在最优控制理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。
在強化学习问题中，智能體（agent）與环境的交互通常被抽象为马尔可夫决策过程（Markov decision processes，MDP），因为很多强化学习算法在这种假设下才能使用动态规划的方法。传统的动态规划方法和强化学习算法的主要区别是，后者不需要关于MDP的知识，而且针对无法找到确切方法的大规模MDP。
对于时刻 t {\displaystyle t} 下觀測（observation） o t ∈ O {\displaystyle o_{t}\in O} 與環境實際狀態 s t ∈ S {\displaystyle s_{t}\in S} 的關係，MDP可以被分為： 完全可观测MDP，若 o t = s…